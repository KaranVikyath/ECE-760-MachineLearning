{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import time\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torch import optim\n"
      ],
      "metadata": {
        "id": "i-MiIvWojDLG"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "SgSQnHqaMqth"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,)),])\n",
        "input_size = 784\n",
        "hidden_layer_size = 300\n",
        "output_size = 10\n",
        "losses = []\n",
        "accuracies = []\n",
        "\n",
        "class FromScratchModel():\n",
        "    def __init__(self, sizes, epochs=10, alpha=0.01):\n",
        "        self.sizes = sizes\n",
        "        self.epochs = epochs\n",
        "        self.alpha = alpha\n",
        "        self.init_w()\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def softmax(self, x):\n",
        "        expo = np.exp(x)\n",
        "        denominator = np.sum(expo, axis=1)\n",
        "        denominator.resize(expo.shape[0], 1)\n",
        "        return expo / denominator\n",
        "\n",
        "    def init_w(self):\n",
        "        input_layer = int(self.sizes[0])\n",
        "        hidden_layer_1 = int(self.sizes[1])\n",
        "        hidden_layer_2 = int(self.sizes[2])\n",
        "        output_layer = int(self.sizes[3])\n",
        "        self.w1 = np.random.uniform(low=-1, high=1, size=(input_layer, hidden_layer_1))\n",
        "        self.w2 = np.random.uniform(low=-1, high=1, size=(hidden_layer_1, hidden_layer_2))\n",
        "        self.w3 = np.random.uniform(low=-1, high=1, size=(hidden_layer_2, output_layer))\n",
        "\n",
        "    def forward_propagation(self, inputs):\n",
        "        input = inputs.numpy()\n",
        "        self.linear_1 = input.dot(self.w1)\n",
        "        self.output_1 = self.sigmoid(self.linear_1)\n",
        "        self.linear_2 = self.output_1.dot(self.w2)\n",
        "        self.output_2 = self.sigmoid(self.linear_2)\n",
        "        self.linear_3=self.output_2.dot(self.w3)\n",
        "        self.output_3 = self.softmax(self.linear_3)\n",
        "        return self.output_3\n",
        "\n",
        "    def backward_propagation(self, X_train, y_train, output):\n",
        "        X_train = X_train.numpy()\n",
        "        y_train = y_train.numpy()\n",
        "        batch_size = y_train.shape[0]\n",
        "        pred_loss = output - y_train\n",
        "        grad_w3 = (1. / batch_size) * np.matmul(self.output_2.T, pred_loss)\n",
        "        d_output_1 = np.matmul(pred_loss, self.w3.T)\n",
        "        d_linear_2 = d_output_1 * self.sigmoid(self.linear_2) * (1 - self.sigmoid(self.linear_2))\n",
        "        grad_w2 = (1. / batch_size) * np.matmul(self.output_1.T, d_linear_2)\n",
        "        d_output_2 = np.matmul(d_linear_2, self.w2.T)\n",
        "        d_linear_1 = d_output_2 * self.sigmoid(self.linear_1) * (1 - self.sigmoid(self.linear_1))\n",
        "        grad_w1 = (1. / batch_size) * np.matmul(X_train.T, d_linear_1)\n",
        "        return grad_w1, grad_w2 ,grad_w3\n",
        "\n",
        "    def weight_update(self, w1_old, w2_old, w3_old):\n",
        "        self.w1 -= self.alpha * w1_old\n",
        "        self.w2 -= self.alpha * w2_old\n",
        "        self.w3 -= self.alpha * w3_old\n",
        "\n",
        "    def calc_loss(self, y, y_pred):\n",
        "        batch_size = y.shape[0]\n",
        "        y = y.numpy()\n",
        "        loss = np.sum(np.multiply(y, np.log(y_pred)))\n",
        "        loss = -(1. / batch_size) * loss\n",
        "        return loss\n",
        "\n",
        "    def calc_metrics(self, test):\n",
        "        losses = []\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for i, data in enumerate(test):\n",
        "            x, y = data\n",
        "            y_onehot_encode = torch.zeros(y.shape[0], 10)\n",
        "            y_onehot_encode[range(y_onehot_encode.shape[0]), y] = 1\n",
        "            flattened_input = x.view(-1, 28 * 28)\n",
        "            output = self.forward_propagation(flattened_input)\n",
        "            y_pred = np.argmax(output, axis=1)\n",
        "            correct += np.sum((y_pred == y.numpy()))\n",
        "            total += y.shape[0]\n",
        "            loss = self.calc_loss(y_onehot_encode, output)\n",
        "            losses.append(loss)\n",
        "        return (correct / total), np.mean(np.array(losses))\n",
        "\n",
        "    def training(self, train, test):\n",
        "        start_time = time.time()\n",
        "        global losses, accuracies\n",
        "        for i in range(self.epochs):\n",
        "            for j, data in enumerate(train):\n",
        "                x, y = data\n",
        "                y_onehot_encode = torch.zeros(y.shape[0], 10)\n",
        "                y_onehot_encode[range(y_onehot_encode.shape[0]), y] = 1\n",
        "                flattened_input = x.view(-1, 28 * 28)\n",
        "                output = self.forward_propagation(flattened_input)\n",
        "                w1_up, w2_up, w3_up = self.backward_propagation(flattened_input, y_onehot_encode, output)\n",
        "                self.weight_update(w1_up, w2_up,w3_up)\n",
        "            accuracy, loss = self.calc_metrics(test)\n",
        "            losses.append(loss)\n",
        "            accuracies.append(accuracy)\n",
        "            print('Epoch: {0}, Test Error Percent: {1:.2f}, Loss: {2:.2f}'.format(i + 1, 100 - accuracy * 100, loss))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2 Case 1"
      ],
      "metadata": {
        "id": "HPGATanb1AH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "α = 0.05 \n",
        "BatchSize = 32 \n",
        "Epochs = 20 \n",
        "\"\"\"\n",
        "\n",
        "model = FromScratchModel(sizes=[784, 300, 200, 10], epochs=20,  alpha=0.05)\n",
        "bsize=32\n",
        "trainset = datasets.MNIST('./dataset/MNIST/', download=True, train=True, transform=transform)\n",
        "testset = datasets.MNIST('./dataset/MNIST/', download=True, train=False, transform=transform)\n",
        "train = torch.utils.data.DataLoader(trainset, batch_size=bsize, shuffle=True)\n",
        "test = torch.utils.data.DataLoader(testset, batch_size=bsize, shuffle=True)\n",
        "model.training(train, test)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Test Loss')\n",
        "plt.plot(losses)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NVx25n4MzUFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2 Case 2"
      ],
      "metadata": {
        "id": "ywhlBO1d1Cyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "α = 0.05 \n",
        "BatchSize = 64\n",
        "Epochs = 20 \n",
        "\"\"\"\n",
        "model = FromScratchModel(sizes=[784, 300, 200, 10], epochs=20,  alpha=0.05)\n",
        "bsize=64\n",
        "train = torch.utils.data.DataLoader(trainset, batch_size=bsize, shuffle=True)\n",
        "test = torch.utils.data.DataLoader(testset, batch_size=bsize, shuffle=True)\n",
        "model.training(train, test)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Test Loss')\n",
        "plt.plot(losses)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pyYieN161Ehz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2 Case 3"
      ],
      "metadata": {
        "id": "qzdZn_U41G11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "α = 0.005 \n",
        "BatchSize = 32 \n",
        "Epochs = 20 \n",
        "\"\"\"\n",
        "model = FromScratchModel(sizes=[784, 300, 200, 10], epochs=20, alpha=0.005)\n",
        "bsize=32\n",
        "train = torch.utils.data.DataLoader(trainset, batch_size=bsize, shuffle=True)\n",
        "test = torch.utils.data.DataLoader(testset, batch_size=bsize, shuffle=True)\n",
        "model.training(train, test)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Test Loss')\n",
        "plt.plot(losses)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XeIRE28O1HsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3 "
      ],
      "metadata": {
        "id": "ek8E_hnD54Zd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def PytorchModel(bsize,rate,epochs=10):\n",
        "  transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,))])\n",
        "  training_set = torchvision.datasets.MNIST('data', train=True, transform=transform, download=True)\n",
        "  train = torch.utils.data.DataLoader(training_set, batch_size=bsize, shuffle=True)\n",
        "\n",
        "  test_set = torchvision.datasets.MNIST('data', train=True, transform=transform, download=True)\n",
        "  test = torch.utils.data.DataLoader(test_set, batch_size=bsize, shuffle=True)\n",
        "\n",
        "  input_size = train.dataset.train_data.shape[1] * train.dataset.train_data.shape[2]\n",
        "  hidden_layers = [300,200]\n",
        "  output_size = 10\n",
        "\n",
        "  def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "      # torch.nn.init.uniform_(m.weight,-1.0,1.0) #random btw -1 and 1\n",
        "      torch.nn.init.zeros_(m.weight) #0 weight initilization\n",
        "\n",
        "\n",
        "  m = nn.Sequential(\n",
        "      nn.Linear(input_size, hidden_layers[0]),\n",
        "      nn.Sigmoid(),\n",
        "      nn.Linear(hidden_layers[0], hidden_layers[1]),\n",
        "      nn.Sigmoid(),\n",
        "      nn.Linear(hidden_layers[1], output_size),\n",
        "      nn.LogSoftmax(dim=1)\n",
        "  )\n",
        "  m.apply(init_weights)\n",
        "  print(m)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(m.parameters(), lr=rate)\n",
        "\n",
        "  losses = []\n",
        "  for i in range(epochs):\n",
        "      run_loss = 0\n",
        "      for x, y in train:\n",
        "          \n",
        "          x = x.view(x.shape[0], -1)\n",
        "          optimizer.zero_grad()        \n",
        "          output = m(x)\n",
        "          loss = criterion(output, y)\n",
        "          \n",
        "          loss.backward() \n",
        "          \n",
        "          optimizer.step()\n",
        "          run_loss += loss.item()\n",
        "      else:\n",
        "          print(\"Epoch: \",i+1)\n",
        "          print(\"Run loss: \",(run_loss/len(train)))\n",
        "          losses.append(run_loss/len(train))\n",
        "\n",
        "\n",
        "  correct=0\n",
        "  with torch.no_grad():\n",
        "    for images,labels in test:\n",
        "      log_p = m(images.view(images.shape[0], -1))\n",
        "      output = torch.squeeze(log_p)\n",
        "      y_pred = output.data.max(1, keepdim=True)[1]\n",
        "      correct += y_pred.eq(labels.data.view_as(y_pred)).sum()\n",
        "    print('\\nAccuracy Percent: {}/{} ({:.0f})\\n'.format(correct, len(test.dataset),100. * correct / len(test.dataset)))\n",
        "    print('\\nTest Error Percent: ({:.0f})\\n'.format(100 - 100. * correct / len(test.dataset)))  \n",
        "\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Test Loss')\n",
        "  plt.plot(losses)\n",
        "  plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EfZjNya2zlL1"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3 Case 1"
      ],
      "metadata": {
        "id": "eR7amnpnPr1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "α = 0.05\n",
        "BatchSize = 32\n",
        "Epochs = 20\n",
        "Initialised randomly between -1 and 1\n",
        "\"\"\"\n",
        "PytorchModel(32,0.05,epochs=20)"
      ],
      "metadata": {
        "id": "iQbVmx-QPsKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3 Case 2"
      ],
      "metadata": {
        "id": "xpWBnEpP7elQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "α = 0.05\n",
        "BatchSize = 64\n",
        "Epochs = 20\n",
        "Initialised randomly between -1 and 1\n",
        "\"\"\"\n",
        "PytorchModel(64,0.05,epochs=20)"
      ],
      "metadata": {
        "id": "GDx9wKla7g9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3 Case 3"
      ],
      "metadata": {
        "id": "-seET1xl7jze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "α = 0.005\n",
        "BatchSize = 32\n",
        "Epochs = 20\n",
        "Initialised randomly between -1 and 1\n",
        "\"\"\"\n",
        "PytorchModel(32,0.005,epochs=20)"
      ],
      "metadata": {
        "id": "pfS71fIU7lKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4 Case 1"
      ],
      "metadata": {
        "id": "RsaNDG-KAaVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "α = 0.05, \n",
        "BatchSize = 32, \n",
        "Epochs = 20, \n",
        "Initialised 0\n",
        "\"\"\"\n",
        "\n",
        "PytorchModel(32,0.01,epochs = 20)"
      ],
      "metadata": {
        "id": "RtswdPOv52Bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4 Case 2"
      ],
      "metadata": {
        "id": "EBfcZ_G_AheA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "α = 0.05\n",
        "BatchSize = 32\n",
        "Epochs = 20\n",
        "Initialised randomly between -1 and 1\n",
        "\"\"\"\n",
        "\n",
        "PytorchModel(64,0.01,epochs=20)"
      ],
      "metadata": {
        "id": "Vul-M3exKCB6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}